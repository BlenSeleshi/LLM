{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BlenSeleshi/LLM/blob/task-1/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRIdFrw_PwRd"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets\n",
        "!pip install datasets\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S3kP4d6PUmo"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import os\n",
        "\n",
        "def read_conll_file(filepath):\n",
        "    tokens = []\n",
        "    labels = []\n",
        "    current_sentence_tokens = []\n",
        "    current_sentence_labels = []\n",
        "\n",
        "    with open(filepath, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip() == \"\":  # End of sentence\n",
        "                if current_sentence_tokens:\n",
        "                    tokens.append(current_sentence_tokens)\n",
        "                    labels.append(current_sentence_labels)\n",
        "                    current_sentence_tokens = []\n",
        "                    current_sentence_labels = []\n",
        "            else:\n",
        "                token, label = line.split()\n",
        "                current_sentence_tokens.append(token)\n",
        "                current_sentence_labels.append(label)\n",
        "\n",
        "    if current_sentence_tokens:  # Add last sentence if file doesn't end with newline\n",
        "        tokens.append(current_sentence_tokens)\n",
        "        labels.append(current_sentence_labels)\n",
        "\n",
        "    return tokens, labels\n",
        "\n",
        "tokens, labels = read_conll_file('merged_output.conll')\n",
        "\n",
        "dataset = Dataset.from_dict({'tokens': tokens, 'ner_tags': labels})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDAdzg_KcOs0"
      },
      "outputs": [],
      "source": [
        "\n",
        "label_normalization = {\n",
        "    'O': 'O',\n",
        "    \"'O'\": 'O',\n",
        "    \"'O']\":'O',\n",
        "    'IO': 'O',\n",
        "    'I-PHONE': 'O',\n",
        "    \"B-PRICE\": 'B-PRICE',\n",
        "    \"B-Price\": 'B-PRICE',\n",
        "    \"I-PRICE\": 'I-PRICE',\n",
        "    \"I-Price\": 'I-PRICE',\n",
        "    \"B-LOC\": 'B-LOC',\n",
        "    \"I-LOC\": 'I-LOC',\n",
        "    \"B-PRODUCT\": 'B-PRODUCT',\n",
        "    \"I-PRODUCT\": 'I-PRODUCT',\n",
        "    \"B-Product\": 'B-PRODUCT',\n",
        "    \"I-Product\": 'I-PRODUCT',\n",
        "    \"'I-LOC']\":'I-LOC',\n",
        "    \"'I-PRODUCT']\":'I-PRODUCT',\n",
        "    \"'I-PRICE']\":'I-PRICE',\n",
        "    \"'B-PRODUCT']\":'B-PRODUCT',\n",
        "    \"'B-PRICE']\":'B-PRICE'\n",
        "\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "label_list = list(set(label_normalization.values()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNwAIF5hWcxo"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples['tokens'], padding='max_length', truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['ner_tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)  # Special tokens (CLS, SEP)\n",
        "            elif word_idx != previous_word_idx:  # Start of a new word\n",
        "                # Normalize the label using the normalization mapping\n",
        "                normalized_label = label_normalization.get(label[word_idx], 'O')  # Default to 'O' if not found\n",
        "                label_ids.append(label_list.index(normalized_label))\n",
        "            else:\n",
        "                label_ids.append(-100)  # Pad subword tokens with -100\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs['labels'] = labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Reapply the function\n",
        "tokenized_datasets = dataset.map(tokenize_and_align_labels, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zuog8_2XeU05"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_size = len(tokenized_datasets)\n",
        "\n",
        "\n",
        "train_size = int(0.8 * dataset_size)\n",
        "eval_size = dataset_size - train_size\n",
        "\n",
        "\n",
        "small_train_dataset = tokenized_datasets.shuffle(seed=42).select(range(train_size))\n",
        "small_eval_dataset = tokenized_datasets.shuffle(seed=42).select(range(train_size, train_size + eval_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyRFmntDecyR"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    \"bert-base-cased\",\n",
        "    num_labels=len(label_list)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install evaluate"
      ],
      "metadata": {
        "id": "Hx2CyTE9f5bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now proceed with the rest of the code\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "# Load the evaluation metric\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "# Define compute_metrics function for accuracy\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for l in label if l != -100]\n",
        "        for label in labels\n",
        "    ]\n",
        "\n",
        "\n",
        "    pred_indices = [[label_list.index(p) for p in pred] for pred in true_predictions]\n",
        "    label_indices = [[label_list.index(l) for l in lab] for lab in true_labels]\n",
        "\n",
        "    return metric.compute(predictions=pred_indices, references=label_indices)\n",
        "\n"
      ],
      "metadata": {
        "id": "qw4-1uUwfnkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    eval_strategy=\"epoch\",  # Use eval_strategy instead of evaluation_strategy to avoid the warning\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "Zkb4uvheh2Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "_eWh3t5-jl2u",
        "outputId": "09a734cf-fdd0-497a-a26a-b3b4c35241da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2685' max='8052' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2685/8052 1:07:26 < 2:14:55, 0.66 it/s, Epoch 1/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='152' max='671' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [152/671 01:13 < 04:11, 2.06 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3b2YTaM6E8s7z/NnEgi3i",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}